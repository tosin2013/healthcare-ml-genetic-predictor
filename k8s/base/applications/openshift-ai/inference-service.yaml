apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: genetic-risk-predictor
  labels:
    app.kubernetes.io/component: openshift-ai
    app.kubernetes.io/name: genetic-risk-ml
    app.kubernetes.io/part-of: healthcare-ml-genetic-predictor
    app.kubernetes.io/version: "1.0.0"
    opendatahub.io/component: "true"
  annotations:
    insights.openshift.io/billing-model: chargeback
    insights.openshift.io/cost-center: genomics-research
    insights.openshift.io/project: genetic-ml-v1
    openshift.io/display-name: "Genetic Risk Inference Service"
    openshift.io/description: "High-performance inference service for genetic risk prediction"
    serving.kserve.io/deploymentMode: Serverless
    autoscaling.knative.dev/class: kpa.autoscaling.knative.dev
    autoscaling.knative.dev/metric: concurrency
    autoscaling.knative.dev/target: "10"
    autoscaling.knative.dev/targetUtilizationPercentage: "70"
    autoscaling.knative.dev/minScale: "0"
    autoscaling.knative.dev/maxScale: "10"
    autoscaling.knative.dev/scaleToZeroGracePeriod: "30s"
    autoscaling.knative.dev/scaleDownDelay: "0s"
    autoscaling.knative.dev/stableWindow: "60s"
spec:
  predictor:
    serviceAccountName: genetic-ml-pipeline
    model:
      modelFormat:
        name: sklearn
        version: "1"
      runtime: mlserver-1.x
      runtimeVersion: "1.3.5"
      protocolVersion: v2
      storage:
        key: genetic-models-storage
        path: genetic-risk-model
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
        limits:
          cpu: "2"
          memory: "4Gi"
      env:
        - name: MLSERVER_MODEL_NAME
          value: genetic-risk-predictor
        - name: MLSERVER_MODEL_URI
          value: /mnt/models/genetic-risk-model
        - name: MLSERVER_MODEL_IMPLEMENTATION
          value: mlserver_sklearn.SKLearnModel
        - name: MLSERVER_HTTP_PORT
          value: "8080"
        - name: MLSERVER_GRPC_PORT
          value: "8081"
        - name: MLSERVER_METRICS_PORT
          value: "8082"
        - name: MLSERVER_DEBUG
          value: "false"
        - name: MLSERVER_PARALLEL_WORKERS
          value: "1"
        - name: MLSERVER_MAX_BUFFER_SIZE
          value: "104857600"  # 100MB
      # Health checks
      livenessProbe:
        httpGet:
          path: /v2/health/live
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      readinessProbe:
        httpGet:
          path: /v2/health/ready
          port: 8080
        initialDelaySeconds: 10
        periodSeconds: 5
        timeoutSeconds: 3
        failureThreshold: 3
  # Optional: Transformer for preprocessing
  transformer:
    containers:
      - name: genetic-data-transformer
        image: python:3.11-slim
        command:
          - python
          - -c
          - |
            import json
            import sys
            from http.server import HTTPServer, BaseHTTPRequestHandler
            import urllib.request
            
            class TransformerHandler(BaseHTTPRequestHandler):
                def do_POST(self):
                    content_length = int(self.headers['Content-Length'])
                    post_data = self.rfile.read(content_length)
                    
                    try:
                        # Parse input data
                        input_data = json.loads(post_data.decode('utf-8'))
                        
                        # Transform VEP annotations to model features
                        transformed_data = self.transform_vep_to_features(input_data)
                        
                        # Send to model
                        response_data = self.call_model(transformed_data)
                        
                        self.send_response(200)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        self.wfile.write(json.dumps(response_data).encode())
                    except Exception as e:
                        self.send_response(500)
                        self.end_headers()
                        self.wfile.write(json.dumps({"error": str(e)}).encode())
                
                def transform_vep_to_features(self, vep_data):
                    # Transform VEP annotations to ML features
                    features = {
                        "sequence_length": len(vep_data.get("sequence", "")),
                        "variant_count": len(vep_data.get("annotations", [])),
                        "consequence_severity": self.calculate_severity(vep_data),
                        "gene_impact": self.calculate_gene_impact(vep_data)
                    }
                    return {"instances": [list(features.values())]}
                
                def calculate_severity(self, vep_data):
                    # Simplified severity calculation
                    annotations = vep_data.get("annotations", [])
                    if not annotations:
                        return 0.0
                    
                    severity_scores = {
                        "HIGH": 1.0,
                        "MODERATE": 0.7,
                        "LOW": 0.3,
                        "MODIFIER": 0.1
                    }
                    
                    total_score = sum(severity_scores.get(ann.get("impact", "MODIFIER"), 0.1) 
                                    for ann in annotations)
                    return total_score / len(annotations)
                
                def calculate_gene_impact(self, vep_data):
                    # Simplified gene impact calculation
                    annotations = vep_data.get("annotations", [])
                    unique_genes = set(ann.get("gene_symbol", "") for ann in annotations)
                    return len(unique_genes)
                
                def call_model(self, transformed_data):
                    # Forward to the actual model
                    req = urllib.request.Request(
                        "http://localhost:8080/v2/models/genetic-risk-predictor/infer",
                        data=json.dumps(transformed_data).encode(),
                        headers={'Content-Type': 'application/json'}
                    )
                    with urllib.request.urlopen(req) as response:
                        return json.loads(response.read().decode())
            
            if __name__ == "__main__":
                server = HTTPServer(('0.0.0.0', 8080), TransformerHandler)
                print("Transformer server starting on port 8080...")
                server.serve_forever()
        ports:
          - containerPort: 8080
            name: http
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
