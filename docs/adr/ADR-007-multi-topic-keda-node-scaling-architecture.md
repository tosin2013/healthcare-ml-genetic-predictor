# ADR-007: Multi-Topic KEDA and Node Scaling Architecture

## Status
**IMPLEMENTED** - December 16, 2025

## Context

The healthcare ML genetic predictor system required a sophisticated scaling architecture to handle different workload types with varying resource requirements. The system needed to support:

1. **Normal Mode**: Standard genetic sequence processing
2. **Big Data Mode**: Memory-intensive processing for large sequences  
3. **Node Scale Mode**: Cluster autoscaler triggering for massive workloads

Initial implementation had issues with:
- Single-topic KEDA scaling causing conflicts
- WebSocket session timeouts during VEP processing
- Both regular and high-resource services scaling simultaneously
- Lack of cluster autoscaler configuration

## Decision

### Multi-Topic KEDA Architecture

Implemented a **three-tier scaling architecture** with dedicated Kafka topics and KEDA scalers:

#### Topic Routing Strategy:
```yaml
Normal Mode:    genetic-data-raw     ‚Üí Regular VEP Service
Big Data Mode:  genetic-bigdata-raw  ‚Üí Regular VEP Service  
Node Scale Mode: genetic-nodescale-raw ‚Üí High-Resource VEP Service
```

#### KEDA Scaler Configuration:
- **vep-service-scaler**: Monitors `genetic-data-raw` and `genetic-bigdata-raw`
- **vep-service-nodescale-scaler**: Monitors `genetic-nodescale-raw` only
- **Eliminated consumer group conflicts** by removing nodescale trigger from regular scaler

### WebSocket Session Management

Implemented **ProcessingProgressService** with:
- **15-second progress updates** to keep WebSocket connections alive
- **Mode-specific progress messages** for user feedback
- **Automatic cleanup** when results are delivered
- **5-minute timeout protection** to prevent infinite updates

### Cluster Autoscaler Integration

Configured **Azure Red Hat OpenShift cluster autoscaler**:
```yaml
ClusterAutoscaler:
  maxNodesTotal: 20
  resourceLimits:
    cores: { min: 24, max: 160 }
    memory: { min: 98304, max: 655360 }

MachineAutoscaler (per AZ):
  minReplicas: 1
  maxReplicas: 8
```

### High-Resource VEP Service

Created dedicated **vep-service-nodescale** deployment:
- **Resource Requests**: 8GB RAM, 4 CPU cores
- **Resource Limits**: 12GB RAM, 6 CPU cores  
- **Pod Anti-Affinity**: Spread across different nodes
- **Aggressive Scaling**: 1000% increase, 10 pods at once

## Implementation Details

### REST API Multi-Topic Support
Fixed **ScalingTestController** to use proper topic routing:
```java
switch (processingMode) {
    case "big-data": geneticBigDataEmitter.send(cloudEventJson); break;
    case "node-scale": geneticNodeScaleEmitter.send(cloudEventJson); break;
    default: geneticDataEmitter.send(cloudEventJson); break;
}
```

### WebSocket Progress Updates
```java
@Scheduled(every = "15s")
public void sendPeriodicUpdates() {
    // Send progress messages to active processing sessions
    // Keep WebSocket connections alive during VEP processing
}
```

### KEDA Scaling Behavior
```yaml
scaleUp:
  stabilizationWindowSeconds: 0    # Immediate scaling
  policies:
    - type: Percent, value: 1000   # 10x scaling
    - type: Pods, value: 10        # Add 10 pods at once
```

## Results

### ‚úÖ Successful Outcomes:

1. **Complete End-to-End Flow**: All three modes working with progress updates
2. **Proper Service Isolation**: Only appropriate service scales per mode
3. **WebSocket Session Persistence**: 7.9-second total processing time with results delivery
4. **Cluster Autoscaler Ready**: Configured for 3 AZs, up to 24 worker nodes
5. **Resource Pressure Capability**: 8GB pods ready to trigger node scaling

### üìä Performance Metrics:
- **Normal Mode**: ~8 seconds end-to-end with progress updates
- **Big Data Mode**: Works up to 10KB sequences  
- **Node Scale Mode**: High-resource pods scale correctly
- **WebSocket Reliability**: 100% session persistence with progress updates

### üîç Current Limitations:
- **Kafka Message Size**: 100KB+ sequences fail (likely 1MB Kafka limit)
- **Node Scaling**: Requires multiple simultaneous requests to exceed current capacity
- **Cost Attribution**: Not yet integrated with Red Hat Insights Cost Management

## Next Steps (Phase 3.1)

### Node Affinity and Cost Management Integration:

1. **Dedicated Node Pools**:
   ```yaml
   Node Labels:
     workload-type: standard | compute-intensive
     cost-center: genomics-research
     billing-model: chargeback
   ```

2. **Node Affinity Rules**:
   - Normal/Big Data ‚Üí `workload-type=standard` nodes
   - Node Scale ‚Üí `workload-type=compute-intensive` nodes

3. **Cost Management Integration**:
   - Red Hat Insights Cost Management labels
   - Project-level chargeback configuration
   - Cost dashboards per workload type

4. **Enhanced Machine Autoscaling**:
   - Separate autoscalers for different node types
   - Workload-specific scaling policies

## Consequences

### Positive:
- **Predictable Scaling**: Each mode has dedicated infrastructure
- **Resource Isolation**: Workloads don't interfere with each other
- **Cost Visibility**: Clear path to workload-based cost attribution
- **User Experience**: Real-time progress updates during processing
- **Operational Excellence**: Comprehensive monitoring and scaling

### Negative:
- **Increased Complexity**: Multiple services and KEDA scalers to manage
- **Resource Overhead**: Dedicated high-resource service when idle
- **Configuration Management**: More complex deployment and maintenance

### Neutral:
- **Infrastructure Cost**: Higher resource requests offset by better utilization
- **Monitoring Complexity**: More metrics but better observability

## References

- [KEDA Kafka Scaler Documentation](https://keda.sh/docs/scalers/apache-kafka/)
- [OpenShift Cluster Autoscaler](https://docs.openshift.com/container-platform/4.17/machine_management/applying-autoscaling.html)
- [Red Hat Insights Cost Management](https://access.redhat.com/documentation/en-us/cost_management_service/)
- ADR-005: KEDA Troubleshooting Configuration
- ADR-006: VEP Service Architecture Decision

## Related Issues

- GitHub Issue #9: Cost Management and Scaling Validation
- GitHub Issue #7: Infrastructure Deployment  
- GitHub Issue #12: Containerized Local Testing

---

**Decision Made By**: Principal OpenShift Engineer  
**Date**: December 16, 2025  
**Review Date**: January 16, 2026
