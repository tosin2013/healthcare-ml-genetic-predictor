{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Monitoring and Scaling Analysis\n",
    "\n",
    "This notebook demonstrates cost monitoring and scaling analysis for the healthcare ML demo.\n",
    "\n",
    "## Features\n",
    "- Monitor OpenShift resource usage\n",
    "- Track KEDA scaling events\n",
    "- Analyze cost attribution\n",
    "- Visualize scaling patterns\n",
    "- Generate cost reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from IPython.display import clear_output, display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "print(\"üìä Cost Monitoring and Scaling Analysis\")\n",
    "print(\"üîç Monitoring OpenShift resources and KEDA scaling\")\n",
    "print(\"üí∞ Tracking cost attribution and resource usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Resource Monitoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pod_metrics():\n",
    "    \"\"\"Simulate getting pod metrics (in real environment, would use OpenShift API)\"\"\"\n",
    "    # Simulate pod metrics data\n",
    "    pods = [\n",
    "        'quarkus-websocket-service',\n",
    "        'vep-service', \n",
    "        'genetic-analysis-workbench',\n",
    "        'genetic-data-cluster-kafka',\n",
    "        'genetic-data-cluster-zookeeper'\n",
    "    ]\n",
    "    \n",
    "    metrics = []\n",
    "    current_time = datetime.now()\n",
    "    \n",
    "    for pod in pods:\n",
    "        # Simulate varying resource usage\n",
    "        cpu_usage = np.random.uniform(0.1, 2.0)  # CPU cores\n",
    "        memory_usage = np.random.uniform(100, 1000)  # MB\n",
    "        \n",
    "        # VEP service might have higher usage during processing\n",
    "        if pod == 'vep-service':\n",
    "            cpu_usage *= np.random.uniform(1.5, 3.0)\n",
    "            memory_usage *= np.random.uniform(1.2, 2.0)\n",
    "        \n",
    "        metrics.append({\n",
    "            'pod_name': pod,\n",
    "            'timestamp': current_time,\n",
    "            'cpu_cores': round(cpu_usage, 3),\n",
    "            'memory_mb': round(memory_usage, 1),\n",
    "            'status': 'Running'\n",
    "        })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def get_scaling_events():\n",
    "    \"\"\"Simulate KEDA scaling events\"\"\"\n",
    "    events = []\n",
    "    base_time = datetime.now() - timedelta(hours=2)\n",
    "    \n",
    "    # Simulate some scaling events\n",
    "    for i in range(5):\n",
    "        event_time = base_time + timedelta(minutes=i*20 + np.random.randint(0, 10))\n",
    "        \n",
    "        events.append({\n",
    "            'timestamp': event_time,\n",
    "            'scaler': 'kafka-scaler-vep',\n",
    "            'action': 'scale_up' if i % 2 == 0 else 'scale_down',\n",
    "            'from_replicas': np.random.randint(1, 3),\n",
    "            'to_replicas': np.random.randint(2, 5),\n",
    "            'trigger_metric': 'kafka_consumer_lag',\n",
    "            'metric_value': np.random.randint(10, 100)\n",
    "        })\n",
    "    \n",
    "    return events\n",
    "\n",
    "def calculate_cost_attribution(metrics):\n",
    "    \"\"\"Calculate cost attribution based on resource usage\"\"\"\n",
    "    # Cost rates (example rates in USD per hour)\n",
    "    CPU_COST_PER_CORE_HOUR = 0.05\n",
    "    MEMORY_COST_PER_GB_HOUR = 0.01\n",
    "    \n",
    "    cost_data = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "        cpu_cost = metric['cpu_cores'] * CPU_COST_PER_CORE_HOUR\n",
    "        memory_cost = (metric['memory_mb'] / 1024) * MEMORY_COST_PER_GB_HOUR\n",
    "        total_cost = cpu_cost + memory_cost\n",
    "        \n",
    "        cost_data.append({\n",
    "            'pod_name': metric['pod_name'],\n",
    "            'timestamp': metric['timestamp'],\n",
    "            'cpu_cost_per_hour': round(cpu_cost, 4),\n",
    "            'memory_cost_per_hour': round(memory_cost, 4),\n",
    "            'total_cost_per_hour': round(total_cost, 4)\n",
    "        })\n",
    "    \n",
    "    return cost_data\n",
    "\n",
    "# Get current metrics\n",
    "current_metrics = get_pod_metrics()\n",
    "scaling_events = get_scaling_events()\n",
    "cost_data = calculate_cost_attribution(current_metrics)\n",
    "\n",
    "print(\"‚úÖ Resource monitoring functions ready\")\n",
    "print(f\"üìä Current pods monitored: {len(current_metrics)}\")\n",
    "print(f\"üìà Scaling events: {len(scaling_events)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Real-time Resource Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resource usage dashboard\n",
    "df_metrics = pd.DataFrame(current_metrics)\n",
    "df_costs = pd.DataFrame(cost_data)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# CPU Usage by Pod\n",
    "axes[0,0].bar(df_metrics['pod_name'], df_metrics['cpu_cores'], color='skyblue')\n",
    "axes[0,0].set_title('Current CPU Usage by Pod')\n",
    "axes[0,0].set_ylabel('CPU Cores')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Memory Usage by Pod\n",
    "axes[0,1].bar(df_metrics['pod_name'], df_metrics['memory_mb'], color='lightgreen')\n",
    "axes[0,1].set_title('Current Memory Usage by Pod')\n",
    "axes[0,1].set_ylabel('Memory (MB)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Cost per Hour by Pod\n",
    "axes[1,0].bar(df_costs['pod_name'], df_costs['total_cost_per_hour'], color='salmon')\n",
    "axes[1,0].set_title('Cost per Hour by Pod')\n",
    "axes[1,0].set_ylabel('Cost (USD/hour)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Cost Breakdown\n",
    "total_cpu_cost = df_costs['cpu_cost_per_hour'].sum()\n",
    "total_memory_cost = df_costs['memory_cost_per_hour'].sum()\n",
    "axes[1,1].pie([total_cpu_cost, total_memory_cost], \n",
    "              labels=['CPU Cost', 'Memory Cost'],\n",
    "              autopct='%1.1f%%', colors=['orange', 'lightblue'])\n",
    "axes[1,1].set_title('Cost Breakdown')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüí∞ Cost Summary:\")\n",
    "print(f\"   Total Cost per Hour: ${df_costs['total_cost_per_hour'].sum():.4f}\")\n",
    "print(f\"   Daily Cost Estimate: ${df_costs['total_cost_per_hour'].sum() * 24:.2f}\")\n",
    "print(f\"   Monthly Cost Estimate: ${df_costs['total_cost_per_hour'].sum() * 24 * 30:.2f}\")\n",
    "\n",
    "print(\"\\nüìä Resource Summary:\")\n",
    "print(f\"   Total CPU Usage: {df_metrics['cpu_cores'].sum():.2f} cores\")\n",
    "print(f\"   Total Memory Usage: {df_metrics['memory_mb'].sum():.1f} MB\")\n",
    "print(f\"   Average CPU per Pod: {df_metrics['cpu_cores'].mean():.2f} cores\")\n",
    "print(f\"   Average Memory per Pod: {df_metrics['memory_mb'].mean():.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaling Events Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze scaling events\n",
    "df_scaling = pd.DataFrame(scaling_events)\n",
    "\n",
    "if not df_scaling.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Scaling timeline\n",
    "    colors = ['green' if action == 'scale_up' else 'red' for action in df_scaling['action']]\n",
    "    axes[0].scatter(df_scaling['timestamp'], df_scaling['to_replicas'], \n",
    "                   c=colors, s=100, alpha=0.7)\n",
    "    axes[0].set_title('Scaling Events Timeline')\n",
    "    axes[0].set_xlabel('Time')\n",
    "    axes[0].set_ylabel('Replica Count')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Scaling actions distribution\n",
    "    action_counts = df_scaling['action'].value_counts()\n",
    "    axes[1].pie(action_counts.values, labels=action_counts.index, \n",
    "               autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
    "    axes[1].set_title('Scaling Actions Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìà Scaling Analysis:\")\n",
    "    print(f\"   Total Scaling Events: {len(df_scaling)}\")\n",
    "    print(f\"   Scale Up Events: {len(df_scaling[df_scaling['action'] == 'scale_up'])}\")\n",
    "    print(f\"   Scale Down Events: {len(df_scaling[df_scaling['action'] == 'scale_down'])}\")\n",
    "    print(f\"   Average Metric Value: {df_scaling['metric_value'].mean():.1f}\")\n",
    "    print(f\"   Max Replicas: {df_scaling['to_replicas'].max()}\")\n",
    "    print(f\"   Min Replicas: {df_scaling['to_replicas'].min()}\")\n",
    "else:\n",
    "    print(\"üìù No scaling events to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Live Monitoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_monitoring(duration_minutes=5, update_interval_seconds=30):\n",
    "    \"\"\"Live monitoring of resources and costs\"\"\"\n",
    "    print(f\"üî¥ Starting live monitoring for {duration_minutes} minutes...\")\n",
    "    print(f\"üîÑ Updates every {update_interval_seconds} seconds\")\n",
    "    print(\"‚èπÔ∏è Press Ctrl+C to stop\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    end_time = start_time + (duration_minutes * 60)\n",
    "    \n",
    "    monitoring_data = []\n",
    "    \n",
    "    try:\n",
    "        while time.time() < end_time:\n",
    "            # Get current metrics\n",
    "            current_metrics = get_pod_metrics()\n",
    "            current_costs = calculate_cost_attribution(current_metrics)\n",
    "            \n",
    "            # Calculate totals\n",
    "            total_cpu = sum(m['cpu_cores'] for m in current_metrics)\n",
    "            total_memory = sum(m['memory_mb'] for m in current_metrics)\n",
    "            total_cost = sum(c['total_cost_per_hour'] for c in current_costs)\n",
    "            \n",
    "            # Store data point\n",
    "            data_point = {\n",
    "                'timestamp': datetime.now(),\n",
    "                'total_cpu': total_cpu,\n",
    "                'total_memory': total_memory,\n",
    "                'total_cost_per_hour': total_cost,\n",
    "                'pod_count': len(current_metrics)\n",
    "            }\n",
    "            monitoring_data.append(data_point)\n",
    "            \n",
    "            # Clear output and display current status\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            print(f\"üî¥ Live Monitoring - {datetime.now().strftime('%H:%M:%S')}\")\n",
    "            print(f\"‚è±Ô∏è Elapsed: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "            print(f\"üìä Total CPU: {total_cpu:.2f} cores\")\n",
    "            print(f\"üíæ Total Memory: {total_memory:.1f} MB\")\n",
    "            print(f\"üí∞ Cost per Hour: ${total_cost:.4f}\")\n",
    "            print(f\"üèÉ Active Pods: {len(current_metrics)}\")\n",
    "            \n",
    "            # Show trend if we have multiple data points\n",
    "            if len(monitoring_data) > 1:\n",
    "                prev_cost = monitoring_data[-2]['total_cost_per_hour']\n",
    "                cost_change = total_cost - prev_cost\n",
    "                trend = \"üìà\" if cost_change > 0 else \"üìâ\" if cost_change < 0 else \"‚û°Ô∏è\"\n",
    "                print(f\"üìä Cost Trend: {trend} {cost_change:+.4f}\")\n",
    "            \n",
    "            print(f\"\\n‚èπÔ∏è Press Ctrl+C to stop monitoring\")\n",
    "            \n",
    "            # Wait for next update\n",
    "            time.sleep(update_interval_seconds)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è Monitoring stopped by user\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Monitoring completed. Collected {len(monitoring_data)} data points\")\n",
    "    return monitoring_data\n",
    "\n",
    "# Start live monitoring (uncomment to run)\n",
    "# monitoring_data = live_monitoring(duration_minutes=2, update_interval_seconds=10)\n",
    "\n",
    "print(\"üìä Live monitoring function ready\")\n",
    "print(\"üí° Uncomment the line above to start live monitoring\")\n",
    "print(\"üîç This will show real-time resource usage and cost changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cost Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cost_report():\n",
    "    \"\"\"Generate a comprehensive cost report\"\"\"\n",
    "    print(\"üìã Generating Healthcare ML Demo Cost Report\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get current data\n",
    "    metrics = get_pod_metrics()\n",
    "    costs = calculate_cost_attribution(metrics)\n",
    "    scaling = get_scaling_events()\n",
    "    \n",
    "    df_costs = pd.DataFrame(costs)\n",
    "    \n",
    "    # Report sections\n",
    "    print(\"\\nüí∞ COST SUMMARY\")\n",
    "    print(\"-\" * 20)\n",
    "    total_hourly = df_costs['total_cost_per_hour'].sum()\n",
    "    print(f\"Current Hourly Cost: ${total_hourly:.4f}\")\n",
    "    print(f\"Daily Estimate: ${total_hourly * 24:.2f}\")\n",
    "    print(f\"Weekly Estimate: ${total_hourly * 24 * 7:.2f}\")\n",
    "    print(f\"Monthly Estimate: ${total_hourly * 24 * 30:.2f}\")\n",
    "    \n",
    "    print(\"\\nüìä COST BY COMPONENT\")\n",
    "    print(\"-\" * 25)\n",
    "    for _, row in df_costs.iterrows():\n",
    "        print(f\"{row['pod_name']:<30} ${row['total_cost_per_hour']:.4f}/hour\")\n",
    "    \n",
    "    print(\"\\nüìà SCALING IMPACT\")\n",
    "    print(\"-\" * 20)\n",
    "    if scaling:\n",
    "        df_scaling = pd.DataFrame(scaling)\n",
    "        avg_replicas = df_scaling['to_replicas'].mean()\n",
    "        max_replicas = df_scaling['to_replicas'].max()\n",
    "        print(f\"Average Replicas: {avg_replicas:.1f}\")\n",
    "        print(f\"Peak Replicas: {max_replicas}\")\n",
    "        print(f\"Scaling Events: {len(scaling)}\")\n",
    "        \n",
    "        # Estimate scaling cost impact\n",
    "        base_cost = total_hourly\n",
    "        peak_cost = base_cost * (max_replicas / avg_replicas)\n",
    "        print(f\"Peak Cost Estimate: ${peak_cost:.4f}/hour\")\n",
    "    else:\n",
    "        print(\"No scaling events recorded\")\n",
    "    \n",
    "    print(\"\\nüéØ COST OPTIMIZATION RECOMMENDATIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Find highest cost pod\n",
    "    highest_cost_pod = df_costs.loc[df_costs['total_cost_per_hour'].idxmax()]\n",
    "    print(f\"‚Ä¢ Highest cost component: {highest_cost_pod['pod_name']} (${highest_cost_pod['total_cost_per_hour']:.4f}/hour)\")\n",
    "    \n",
    "    # Resource efficiency\n",
    "    df_metrics = pd.DataFrame(metrics)\n",
    "    avg_cpu = df_metrics['cpu_cores'].mean()\n",
    "    if avg_cpu < 0.5:\n",
    "        print(\"‚Ä¢ Consider reducing CPU requests for underutilized pods\")\n",
    "    \n",
    "    print(\"‚Ä¢ Implement scale-to-zero for development workloads\")\n",
    "    print(\"‚Ä¢ Use spot instances for non-critical batch processing\")\n",
    "    print(\"‚Ä¢ Monitor and adjust KEDA scaling thresholds\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Report generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    return {\n",
    "        'total_hourly_cost': total_hourly,\n",
    "        'daily_estimate': total_hourly * 24,\n",
    "        'monthly_estimate': total_hourly * 24 * 30,\n",
    "        'component_costs': df_costs.to_dict('records'),\n",
    "        'scaling_events': len(scaling)\n",
    "    }\n",
    "\n",
    "# Generate cost report\n",
    "cost_report = generate_cost_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
